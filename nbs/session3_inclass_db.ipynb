{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different NLP frameworks that you're likely to encounter. The most popular and widely-used of these are:\n",
    "\n",
    "- ```NLTK``` (Natural Language Toolkit, old-school)\n",
    "- ```UDPipe``` (Neural network based, fast and light, but not super accurate)\n",
    "- ```CoreNLP``` and ```stanza``` (Created by the team at Stanford; academically robust)\n",
    "- ```spaCy``` production-ready, well-documented, state-of-the-art\n",
    "\n",
    "We'll be working with ```spaCy``` in this module, primarily because it's easy and intuitive, and also scales well.\n",
    "\n",
    "First thing we need to do is install ```spaCy``` and the language model that we want to use.\n",
    "\n",
    "From the command line, you should first make sure to run the setup script to install requirements:\n",
    "\n",
    "```shell \n",
    "bash setup.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is import ```spaCy``` __and__ the language model that we want to use.\n",
    "\n",
    "Note that, if you want to use different langauges you want to use different language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spacy NLP class\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model now loaded, we can begin to do some very simple NLP tasks.\n",
    "\n",
    "Here, we create a spaCy object and assign it to the variable ```nlp```. This is the NLP pipeline that will do all our heavy lifting, using the trained model we've specified.\n",
    "\n",
    "Below, you can see what the pipeline does with a bit of sample text. Passing text to the nlp object gives us access to a bunch of properties, including tokens (words), parts of speech, named entities, and so on. Here's we two of them, tokens and entities. These objects, in turn, have certain methods attached to them. A full outline of available methods can be found in the spaCy docs.\n",
    "\n",
    "In this case, for all token objects, let's return the token itself (```token.text```); its part-of-speech tag (```token.pos_```); and the grammatical dependency relations between the tokens (```token.dep_```).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single sentence example\n",
    "input_string = \"My name is Ross and I have family in New York.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Doc object\n",
    "doc = nlp(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Ross and I have family in New York.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(doc)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My\n",
      "name\n",
      "is\n",
      "Ross\n",
      "and\n",
      "I\n",
      "have\n",
      "family\n",
      "in\n",
      "New\n",
      "York\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# tokenizing text\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Trying some more attributes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 My PRON poss Number=Sing|Person=1|Poss=Yes|PronType=Prs\n",
      "1 name NOUN nsubj Number=Sing\n",
      "2 is AUX ROOT Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "3 Ross PROPN attr Number=Sing\n",
      "4 and CCONJ cc ConjType=Cmp\n",
      "5 I PRON nsubj Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "6 have VERB conj Mood=Ind|Tense=Pres|VerbForm=Fin\n",
      "7 family NOUN dobj Number=Sing\n",
      "8 in ADP prep \n",
      "9 New PROPN compound Number=Sing\n",
      "10 York PROPN pobj Number=Sing\n",
      "11 . PUNCT punct PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "# find parts-of-speech and grammatical relations\n",
    "for token in doc:\n",
    "    print(token.i, token.text, token.pos_, token.dep_, token.morph) #adding the underscore after pos gives us POS-tags in abbreviations and not numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NER__\n",
    "\n",
    "Extracting named entities from a ```spaCy``` doc requires an extra step, but nothing too challenging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ross PERSON\n",
      "New York GPE\n"
     ]
    }
   ],
   "source": [
    "# extracting NERs\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Questions:__ \n",
    "\n",
    "1. What range of linguistic features is available beyond what we're looking at here? \n",
    "2. Are the same range of features available for all languages? Compare e.g. English and Danish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count distribution of linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create doc object__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file\n",
    "import os\n",
    "filename = os.path.join(\"..\", \"data\", \"Dickens_Expectations_1861.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a doc object\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "\n",
    "# get named entities\n",
    "for ent in doc.ents:\n",
    "    entities.append(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(entities)) #set() returns only unique entries of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11109\n"
     ]
    }
   ],
   "source": [
    "# count number of adjectives\n",
    "adjective_count = 0\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adjective_count += 1\n",
    "\n",
    "print(adjective_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relative frequency__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454.63"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the relative frequency per 10,000 words\n",
    "relative_freq = (adjective_count/len(doc))*10000\n",
    "\n",
    "round(relative_freq, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating neater outputs using ```pandas```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, all of our output from ```spaCy``` is in the form of lists. If we want to save these, it probably makes sense to have them saved in a more transferable format, such as CSV files or JSONs.\n",
    "\n",
    "One very easy way to do this with Python is by using the dataframe library ```pandas```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spaCy doc\n",
    "doc = nlp(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "for token in doc:\n",
    "    annotations.append((token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRON'),\n",
       " ('name', 'NOUN'),\n",
       " ('is', 'AUX'),\n",
       " ('Ross', 'PROPN'),\n",
       " ('and', 'CCONJ'),\n",
       " ('I', 'PRON'),\n",
       " ('have', 'VERB'),\n",
       " ('family', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('New', 'PROPN'),\n",
       " ('York', 'PROPN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy doc to pandas dataframe\n",
    "data = pd.DataFrame(annotations, \n",
    "                    columns=[\"token\", \"POS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ross</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>have</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>family</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>York</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token    POS\n",
       "0       My   PRON\n",
       "1     name   NOUN\n",
       "2       is    AUX\n",
       "3     Ross  PROPN\n",
       "4      and  CCONJ\n",
       "5        I   PRON\n",
       "6     have   VERB\n",
       "7   family   NOUN\n",
       "8       in    ADP\n",
       "9      New  PROPN\n",
       "10    York  PROPN\n",
       "11       .  PUNCT"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "outpath = os.path.join(\"..\", \"out\", \"annotations.csv\") # not gonna work since there's no folder called out\n",
    "data.to_csv(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "\n",
    "Spend some time exploring and familiarizing yourself with ```spaCy``` and ```pandas```. We'll come back to them quite a lot through this semester, so it will help to have a solid handle on how they function.\n",
    "\n",
    "When you are ready, head over to [Assignment 1](https://classroom.github.com/a/PdNi7nPv) which takes some of the skills you've learned last week and today. The task will be to count how many times certain linguistic features occur accross different documents, and to save those results in a clear and easy-to-understand way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "274bd49d359452a580d03880534f64da91bee428942d75ba8f1392db091f3279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
